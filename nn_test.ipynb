{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset,random_split\n",
    "import torchvision.io as tio\n",
    "from torchvision.io import ImageReadMode\n",
    "import torchvision.transforms as Transforms\n",
    "\n",
    "import cv2\n",
    "import pathlib\n",
    "import train.dice_score as dice_score\n",
    "import os\n",
    "\n",
    "def imagelist(data_path : str, fmt : str):\n",
    "    # 判断目录是否存在\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError('文件夹不存在')\n",
    "    # 读取目录下的所有jpg图像与png图像\n",
    "    image_paths = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith(fmt)] # '.jpg'\n",
    "    # 用torchvision库读取image_paths里所有的图像\n",
    "    # images = [tio.read_image(f) for f in image_paths]\n",
    "    return image_paths\n",
    "    # 将Tensor作为图像展示\n",
    "    '''\n",
    "        print(images[0].shape)\n",
    "        cv2.imshow('image', images[0].permute(1,2,0).numpy())\n",
    "        cv2.waitKey(0)\n",
    "    '''\n",
    "\n",
    "\n",
    "# 自定义数据集类DataSet\n",
    "class SegDataSet(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, target_transform=None):\n",
    "        self.img_raw = imagelist(root_dir, '.jpg')\n",
    "        self.img_mask = imagelist(root_dir, '.png')\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_raw)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_raw = tio.read_image(self.img_raw[idx], mode = ImageReadMode.GRAY)\n",
    "        if((img_raw > 1).any()):\n",
    "            # bias是float的，这里也要转成float\n",
    "            img_raw = img_raw / 255.0\n",
    "        if self.transform:\n",
    "            #print('transform')\n",
    "            img_raw = self.transform(img_raw)\n",
    "        \n",
    "        img_mask = tio.read_image(self.img_mask[idx], mode = ImageReadMode.GRAY)\n",
    "        if self.target_transform:\n",
    "            #print('target_transform')\n",
    "            img_mask = self.target_transform(img_mask).squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'image' : img_raw, \n",
    "            'mask' : img_mask\n",
    "            }\n",
    "\n",
    "# 全局变量\n",
    "paths = {\n",
    "    'train' : 'F:/model_tuning/data/viod/train',\n",
    "    'test'  : 'F:/model_tuning/data/viod/test',\n",
    "    'checkpoint' : 'F:/model_tuning/data/checkpoint/'\n",
    "}\n",
    "image_size=(256,256)\n",
    "batch_size = 1\n",
    "\n",
    "dataset = SegDataSet(paths['train'], \n",
    "        transform=Transforms.Compose([Transforms.Resize(image_size)]),\n",
    "        target_transform=Transforms.Compose([Transforms.Resize(image_size)]))\n",
    "\n",
    "        #dataset = BasicDataset(dir_img, dir_mask, img_scale)\n",
    "\n",
    "    # 2. Split into train / validation partitions\n",
    "val_percent = 0.1\n",
    "n_val = int(len(dataset) * val_percent)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "# 3. Create data loaders\n",
    "# 遇到了多线程问题，将num_workers设为0后不报错了\n",
    "loader_args = dict(batch_size=batch_size, num_workers=0, pin_memory=True)\n",
    "train_loader = DataLoader(train_set, shuffle=True,**loader_args)\n",
    "val_loader = DataLoader(val_set, shuffle=False, drop_last=True,**loader_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.3507, 0.3519, 0.3684,  ..., 0.3960, 0.3791, 0.3663],\n",
       "          [0.3809, 0.3735, 0.3942,  ..., 0.3958, 0.3838, 0.3753],\n",
       "          [0.4030, 0.4267, 0.4446,  ..., 0.3962, 0.3905, 0.3835],\n",
       "          ...,\n",
       "          [0.4745, 0.5062, 0.5327,  ..., 0.4365, 0.4268, 0.4240],\n",
       "          [0.4133, 0.4391, 0.4786,  ..., 0.4324, 0.4270, 0.4255],\n",
       "          [0.4126, 0.4206, 0.4393,  ..., 0.4275, 0.4210, 0.4219]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = next(iter(train_loader))['image']\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch  遵循的是NCHW\n",
    "opencv 遵循的是 HWC\n",
    "所以要交换顺序permute(1，2，0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 1, 256, 256])\n",
      "conv shape: torch.Size([1, 1, 254, 254])\n",
      "norm shape: torch.Size([1, 1, 254, 254])\n",
      "tran shape: torch.Size([1, 1, 256, 256])\n",
      "maxp shape: torch.Size([1, 1, 127, 127])\n",
      "upsa shape: torch.Size([1, 1, 508, 508])\n",
      "padd shape: torch.Size([1, 1, 256, 256])\n",
      "cat shape: torch.Size([1, 2, 256, 256])\n",
      "relu shape: torch.Size([1, 1, 254, 254])\n",
      "flatten shape: torch.Size([1, 254, 254])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# 测试 conv2D batchNorm Maxpool\n",
    "input_ch = 1\n",
    "output_ch = 1\n",
    "input_img = next(iter(train_loader))['image']\n",
    "print('input shape:',input_img.shape)\n",
    "conv = nn.Conv2d(input_ch, output_ch, kernel_size=3, stride=1, padding=0)\n",
    "conv_re = conv(input_img)\n",
    "print('conv shape:',conv_re.shape)\n",
    "batchNorm = nn.BatchNorm2d(output_ch, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "norm_re = batchNorm(conv_re)\n",
    "print('norm shape:',norm_re.shape)\n",
    "\n",
    "# 测试 transposeConv2D maxpool upSample\n",
    "tran2D = nn.ConvTranspose2d(1,1,kernel_size=3, stride=1, padding=0)\n",
    "tran_re = tran2D(norm_re)\n",
    "print('tran shape:',tran_re.shape)\n",
    "\n",
    "maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "max_re = maxpool(norm_re)\n",
    "print('maxp shape:',max_re.shape)\n",
    "\n",
    "upSample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "up_re = upSample(norm_re)\n",
    "print('upsa shape:',up_re.shape)\n",
    "\n",
    "# 测试 pad cat \n",
    "pad_re = F.pad(norm_re, (1,1,1,1), mode='constant', value=0)\n",
    "print('padd shape:',pad_re.shape)\n",
    "cat_re = torch.cat((pad_re, input_img), dim=1)\n",
    "print('cat shape:',cat_re.shape)\n",
    "\n",
    "# 测试ReLu\n",
    "relu_re = F.relu(norm_re)\n",
    "print('relu shape:',relu_re.shape)\n",
    "\n",
    "# 测试flatten\n",
    "flatten_re = relu_re.flatten(0,1)\n",
    "print('flatten shape:',flatten_re.shape)\n",
    "\n",
    "# 显示图像\n",
    "image_show = (relu_re[0].permute(1,2,0)).detach().numpy()\n",
    "cv2.imshow('img',image_show)\n",
    "cv2.waitKey(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape:  torch.Size([1, 256, 256])\n",
      "unique: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  27  30  31  32  34  35  36  37  38  39\n",
      "  40  41  43  46  47  49  50  51  52  54  55  60  61  64  65  66  68  69\n",
      "  70  72  73  74  76  79  80  84  85  87  88  89  92  96  97  99 100 106\n",
      " 107 108 110 111 114 133 145 147 148 150 151 157 163 164 165 170 171 174\n",
      " 179 182 183 184 191 192 200 204 207 211 215 216 218 221 222 228 230 233\n",
      " 234 254]\n",
      "mask shape:  torch.Size([1, 256, 256])\n",
      "mask_sq shape: torch.Size([256, 256])\n",
      "flat_re shape: torch.Size([256, 256])\n",
      "mask_float dtype: torch.uint8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试 mask\n",
    "import numpy as np\n",
    "mask = next(iter(train_loader))['mask']\n",
    "print('mask shape: ',mask.shape)\n",
    "\n",
    "unique_re = np.unique(mask)\n",
    "print('unique:',unique_re)\n",
    "if (mask != 0).any():\n",
    "    mask[mask != 0] = 1\n",
    "\n",
    "# squeeze 移除大小为1的维度，如果指定的维度大小不为1，张量尺寸不变 sqeeze(1):移除第二个维度\n",
    "mask_sq = mask.squeeze(0)# 等同torch.squeeze(mask,1)\n",
    "print('mask_sq shape:',mask_sq.shape)\n",
    "# 判断移除张量维度是否成功\n",
    "if ((mask_sq.shape == mask.shape)):\n",
    "    print('Sqeeze failed')\n",
    "\n",
    "# flatten 把start至end(包含end)维度合并到end上\n",
    "flat_re = torch.flatten(mask,0,1) # (tensor,start = 0,end = -1)\n",
    "print('flat_re shape:',flat_re.shape)\n",
    "\n",
    "# float 转为float32\n",
    "mask_float = mask.float()\n",
    "print('mask_float dtype:',mask.dtype)\n",
    "\n",
    "image_show = (mask.permute(1,2,0)).detach().numpy()\n",
    "cv2.imshow('img',image_show)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
